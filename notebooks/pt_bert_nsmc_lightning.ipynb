{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fe0a98-9ecd-4d6f-a59f-64af292c4ba2",
   "metadata": {},
   "source": [
    "## Fine Tuning BERT Model for Sentiment Analysis \n",
    "* This is a pytorch lightning code for fine tuning a pretrained BERT model (beomi/kcbert-large from Hugging Face). \n",
    "* It uses the Naver Sentiment Movie Corpos (NSMC) datasets for fine-tunning, which is a collection of movie reviews in the Korean Language.\n",
    "* One epoch took approximately 50 minutes to complete on a NVIDIA A100 GPU.\n",
    "* The original code can be found on the GitHub site at: https://github.com/Beomi/KcBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c875ed7-b770-4213-894f-873d7cd8319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./nsmc/ratings_test.txt', <http.client.HTTPMessage at 0x2ade6ac19600>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"./nsmc/ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename =\"./nsmc/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7582aa2-9fd7-4d32-9891-689e9aedf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tdocument\tlabel\n",
      "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
      "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
      "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
      "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
      "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
      "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
      "7797314\t원작의 긴장감을 제대로 살려내지못했다.\t0\n",
      "9443947\t별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\t0\n",
      "7156791\t액션이 없는데도 재미 있는 몇안되는 영화\t1\n"
     ]
    }
   ],
   "source": [
    "!head ./nsmc/ratings_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161e8d20-b3b6-4dc2-a2d6-93033fe79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "#from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "class Arg:\n",
    "    random_seed: int = 42  # Random Seed\n",
    "    pretrained_model: str = 'beomi/kcbert-large'  # Transformers PLM name\n",
    "    pretrained_tokenizer: str = ''  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
    "    auto_batch_size: str = 'power'  # Let PyTorch Lightening find the best batch size \n",
    "    batch_size: int = 0  # Optional, Train/Eval Batch Size. Overrides `auto_batch_size` \n",
    "    lr: float = 5e-6  # Starting Learning Rate\n",
    "    epochs: int = 1  # Max Epochs\n",
    "    max_length: int = 150  # Max Length input size\n",
    "    report_cycle: int = 100  # Report (Train Metrics) Cycle\n",
    "    train_data_path: str = \"nsmc/ratings_train.txt\"  # Train Dataset file \n",
    "    val_data_path: str = \"nsmc/ratings_test.txt\"  # Validation Dataset file \n",
    "    #cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    cpu_workers: int = 4  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    tpu_cores: int = 0  # Enable TPU with 1 core or 8 cores\n",
    "\n",
    "args = Arg()\n",
    "\n",
    "# args.tpu_cores = 8  # Enables TPU\n",
    "# args.fp16 = True  # Enables GPU FP16\n",
    "args.batch_size = 128  # Force setup batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ace947-79b4-45a0-81dd-7e2c2cd9c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        self.args = options\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(self.args.pretrained_model)\n",
    "        self.validation_step_outputs = []\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            self.args.pretrained_tokenizer\n",
    "            if self.args.pretrained_tokenizer\n",
    "            else self.args.pretrained_model\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.bert(**kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred = preds.cpu().numpy()\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'train_loss': loss.cpu().detach().numpy().tolist(),\n",
    "            'train_acc': metrics[0],\n",
    "            'train_precision': metrics[1],\n",
    "            'train_recall': metrics[2],\n",
    "            'train_f1': metrics[3],\n",
    "        }\n",
    "        if (batch_idx % self.args.report_cycle) == 0:\n",
    "            print()\n",
    "            pprint(tensorboard_logs)\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "   \n",
    "        self.validation_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for i in self.validation_step_outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(self.validation_step_outputs)\n",
    "\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.validation_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': metrics[0],\n",
    "            'val_precision': metrics[1],\n",
    "            'val_recall': metrics[2],\n",
    "            'val_f1': metrics[3],\n",
    "        }\n",
    "\n",
    "        print()\n",
    "        pprint(tensorboard_logs)\n",
    "        self.validation_step_outputs.clear()\n",
    "  \n",
    "        return {'loss': _loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.args.lr)\n",
    "        elif self.args.optimizer == 'AdamP':\n",
    "            from adamp import AdamP\n",
    "            optimizer = AdamP(self.parameters(), lr=self.args.lr)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
    "        if self.args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        elif self.args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def read_data(self, path):\n",
    "        if path.endswith('xlsx'):\n",
    "            return pd.read_excel(path)\n",
    "        elif path.endswith('csv'):\n",
    "            return pd.read_csv(path)\n",
    "        elif path.endswith('tsv') or path.endswith('txt'):\n",
    "            return pd.read_csv(path, sep='\\t')\n",
    "        else:\n",
    "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
    "\n",
    "    def preprocess_dataframe(self, df):\n",
    "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "        url_pattern = re.compile(\n",
    "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "        def clean(x):\n",
    "            x = pattern.sub(' ', x)\n",
    "            x = url_pattern.sub('', x)\n",
    "            x = x.strip()\n",
    "            x = repeat_normalize(x, num_repeats=2)\n",
    "            return x\n",
    "\n",
    "        df['document'] = df['document'].map(lambda x: self.tokenizer.encode(\n",
    "            clean(str(x)),\n",
    "            padding='max_length',\n",
    "            max_length=self.args.max_length,\n",
    "            truncation=True,\n",
    "        ))\n",
    "        return df\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        df = self.read_data(self.args.train_data_path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size or self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        df = self.read_data(self.args.val_data_path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size or self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0afd1a-bd34-4d67-a279-1702c8100159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Ver 1.13.0\n",
      "Fix Seed: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-large were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory logs/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:360: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert | BertForSequenceClassification | 334 M \n",
      "-------------------------------------------------------\n",
      "334 M     Trainable params\n",
      "0         Non-trainable params\n",
      "334 M     Total params\n",
      "1,337.569 Total estimated model params size (MB)\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e5a01bc26e4cbfbf8c980fa36b90bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'train_acc': 0.5234375,\n",
      " 'train_f1': 0.6737967914438502,\n",
      " 'train_loss': 0.754667341709137,\n",
      " 'train_precision': 0.525,\n",
      " 'train_recall': 0.9402985074626866}\n",
      "\n",
      "{'train_acc': 0.8515625,\n",
      " 'train_f1': 0.8671328671328671,\n",
      " 'train_loss': 0.3358471691608429,\n",
      " 'train_precision': 0.8493150684931506,\n",
      " 'train_recall': 0.8857142857142857}\n",
      "\n",
      "{'train_acc': 0.8828125,\n",
      " 'train_f1': 0.8920863309352518,\n",
      " 'train_loss': 0.2869292199611664,\n",
      " 'train_precision': 0.8857142857142857,\n",
      " 'train_recall': 0.8985507246376812}\n",
      "\n",
      "{'train_acc': 0.796875,\n",
      " 'train_f1': 0.8030303030303031,\n",
      " 'train_loss': 0.40201738476753235,\n",
      " 'train_precision': 0.8412698412698413,\n",
      " 'train_recall': 0.7681159420289855}\n",
      "\n",
      "{'train_acc': 0.90625,\n",
      " 'train_f1': 0.9016393442622952,\n",
      " 'train_loss': 0.28917643427848816,\n",
      " 'train_precision': 0.9016393442622951,\n",
      " 'train_recall': 0.9016393442622951}\n",
      "\n",
      "{'train_acc': 0.8984375,\n",
      " 'train_f1': 0.8907563025210085,\n",
      " 'train_loss': 0.24824129045009613,\n",
      " 'train_precision': 0.8833333333333333,\n",
      " 'train_recall': 0.8983050847457628}\n",
      "\n",
      "{'train_acc': 0.875,\n",
      " 'train_f1': 0.8750000000000001,\n",
      " 'train_loss': 0.22696428000926971,\n",
      " 'train_precision': 0.8888888888888888,\n",
      " 'train_recall': 0.8615384615384616}\n",
      "\n",
      "{'train_acc': 0.9296875,\n",
      " 'train_f1': 0.935251798561151,\n",
      " 'train_loss': 0.16673924028873444,\n",
      " 'train_precision': 0.9420289855072463,\n",
      " 'train_recall': 0.9285714285714286}\n",
      "\n",
      "{'train_acc': 0.8515625,\n",
      " 'train_f1': 0.8480000000000001,\n",
      " 'train_loss': 0.27552443742752075,\n",
      " 'train_precision': 0.8983050847457628,\n",
      " 'train_recall': 0.803030303030303}\n",
      "\n",
      "{'train_acc': 0.9296875,\n",
      " 'train_f1': 0.9387755102040817,\n",
      " 'train_loss': 0.20577645301818848,\n",
      " 'train_precision': 0.971830985915493,\n",
      " 'train_recall': 0.9078947368421053}\n",
      "\n",
      "{'train_acc': 0.875,\n",
      " 'train_f1': 0.894736842105263,\n",
      " 'train_loss': 0.2545030415058136,\n",
      " 'train_precision': 0.9066666666666666,\n",
      " 'train_recall': 0.8831168831168831}\n",
      "\n",
      "{'train_acc': 0.84375,\n",
      " 'train_f1': 0.8461538461538461,\n",
      " 'train_loss': 0.3670918643474579,\n",
      " 'train_precision': 0.859375,\n",
      " 'train_recall': 0.8333333333333334}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'val_acc': 0.89726,\n",
      " 'val_f1': 0.8992409234450698,\n",
      " 'val_loss': 0.24743244051933289,\n",
      " 'val_precision': 0.8881441301820999,\n",
      " 'val_recall': 0.9106185198426886}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using PyTorch Ver\", torch.__version__)\n",
    "print(\"Fix Seed:\", args.random_seed)\n",
    "seed_everything(args.random_seed)\n",
    "model = Model(args)\n",
    "\n",
    "print(\":: Start Training ::\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        accelerator=\"auto\",\n",
    "        #accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        #strategy=\"ddp_notebook\",\n",
    "        devices=torch.cuda.device_count() if torch.cuda.is_available() else None,\n",
    "        #devices=\"auto\",\n",
    "        #precision=16 if args.fp16 else 32,\n",
    "        # For TPU Setup\n",
    "        # tpu_cores=args.tpu_cores if args.tpu_cores else None,\n",
    "        logger=CSVLogger(save_dir=\"logs/\")\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed02c1-9344-44c8-b87f-f1e08d5b3785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
