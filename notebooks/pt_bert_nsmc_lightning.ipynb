{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fe0a98-9ecd-4d6f-a59f-64af292c4ba2",
   "metadata": {},
   "source": [
    "## Fine Tuning BERT Model for Sentiment Analysis \n",
    "* This is a pytorch lightning code for fine tuning a pretrained BERT model (beomi/kcbert-large from Hugging Face). \n",
    "* It uses the Naver Sentiment Movie Corpos (NSMC) datasets for fine-tunning, which is a collection of movie reviews in the Korean Language.\n",
    "* One epoch will take approximately 45 minutes to complete on a NVIDIA A100 GPU. You could notice that with the fp16 flag on, it will take less than 10 minutes without losing any accuracy.\n",
    "* The original code can be found on the GitHub site at: https://github.com/Beomi/KcBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c875ed7-b770-4213-894f-873d7cd8319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib.request\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"./nsmc/ratings_train.txt\")\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename =\"./nsmc/ratings_test.txt\")\n",
    "import os\n",
    "if not os.path.exists('./nsmc'):\n",
    "    !git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7582aa2-9fd7-4d32-9891-689e9aedf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tdocument\tlabel\n",
      "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
      "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
      "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
      "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
      "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
      "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
      "7797314\t원작의 긴장감을 제대로 살려내지못했다.\t0\n",
      "9443947\t별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\t0\n",
      "7156791\t액션이 없는데도 재미 있는 몇안되는 영화\t1\n"
     ]
    }
   ],
   "source": [
    "!head ./nsmc/ratings_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161e8d20-b3b6-4dc2-a2d6-93033fe79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "#from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "class Arg:\n",
    "    random_seed: int = 42  # Random Seed\n",
    "    pretrained_model: str = 'beomi/kcbert-large'  # Transformers PLM name\n",
    "    pretrained_tokenizer: str = ''  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
    "    auto_batch_size: str = 'power'  # Let PyTorch Lightening find the best batch size \n",
    "    batch_size: int = 0  # Optional, Train/Eval Batch Size. Overrides `auto_batch_size` \n",
    "    lr: float = 5e-6  # Starting Learning Rate\n",
    "    epochs: int = 1  # Max Epochs\n",
    "    max_length: int = 150  # Max Length input size\n",
    "    report_cycle: int = 100  # Report (Train Metrics) Cycle\n",
    "    train_data_path: str = \"nsmc/ratings_train.txt\"  # Train Dataset file \n",
    "    val_data_path: str = \"nsmc/ratings_test.txt\"  # Validation Dataset file \n",
    "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    #cpu_workers: int = 4  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    tpu_cores: int = 0  # Enable TPU with 1 core or 8 cores\n",
    "\n",
    "args = Arg()\n",
    "\n",
    "# args.tpu_cores = 8  # Enables TPU\n",
    "args.fp16 = True  # Enables GPU FP16\n",
    "args.cpu_workers = 4  # Force setup cpu_workers\n",
    "args.batch_size = 128  # Force setup batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ace947-79b4-45a0-81dd-7e2c2cd9c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        self.args = options\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(self.args.pretrained_model)\n",
    "        self.validation_step_outputs = []\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            self.args.pretrained_tokenizer\n",
    "            if self.args.pretrained_tokenizer\n",
    "            else self.args.pretrained_model\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.bert(**kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred = preds.cpu().numpy()\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'train_loss': loss.cpu().detach().numpy().tolist(),\n",
    "            'train_acc': metrics[0],\n",
    "            'train_precision': metrics[1],\n",
    "            'train_recall': metrics[2],\n",
    "            'train_f1': metrics[3],\n",
    "        }\n",
    "        if (batch_idx % self.args.report_cycle) == 0:\n",
    "            print()\n",
    "            pprint(tensorboard_logs)\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "   \n",
    "        self.validation_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for i in self.validation_step_outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(self.validation_step_outputs)\n",
    "\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.validation_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': metrics[0],\n",
    "            'val_precision': metrics[1],\n",
    "            'val_recall': metrics[2],\n",
    "            'val_f1': metrics[3],\n",
    "        }\n",
    "\n",
    "        print()\n",
    "        pprint(tensorboard_logs)\n",
    "        self.validation_step_outputs.clear()\n",
    "  \n",
    "        return {'loss': _loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.args.lr)\n",
    "        elif self.args.optimizer == 'AdamP':\n",
    "            from adamp import AdamP\n",
    "            optimizer = AdamP(self.parameters(), lr=self.args.lr)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
    "        if self.args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        elif self.args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def read_data(self, path):\n",
    "        if path.endswith('xlsx'):\n",
    "            return pd.read_excel(path)\n",
    "        elif path.endswith('csv'):\n",
    "            return pd.read_csv(path)\n",
    "        elif path.endswith('tsv') or path.endswith('txt'):\n",
    "            return pd.read_csv(path, sep='\\t')\n",
    "        else:\n",
    "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
    "\n",
    "    def preprocess_dataframe(self, df):\n",
    "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "        url_pattern = re.compile(\n",
    "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "        def clean(x):\n",
    "            x = pattern.sub(' ', x)\n",
    "            x = url_pattern.sub('', x)\n",
    "            x = x.strip()\n",
    "            x = repeat_normalize(x, num_repeats=2)\n",
    "            return x\n",
    "\n",
    "        df['document'] = df['document'].map(lambda x: self.tokenizer.encode(\n",
    "            clean(str(x)),\n",
    "            padding='max_length',\n",
    "            max_length=self.args.max_length,\n",
    "            truncation=True,\n",
    "        ))\n",
    "        return df\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        df = self.read_data(self.args.train_data_path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size or self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        df = self.read_data(self.args.val_data_path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size or self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0afd1a-bd34-4d67-a279-1702c8100159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Ver 1.13.0\n",
      "Fix Seed: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-large were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory logs/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:360: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert | BertForSequenceClassification | 334 M \n",
      "-------------------------------------------------------\n",
      "334 M     Trainable params\n",
      "0         Non-trainable params\n",
      "334 M     Total params\n",
      "1,337.569 Total estimated model params size (MB)\n",
      "/scratch/qualis/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e818beef544602b64355d3cc1e2718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'train_acc': 0.5234375,\n",
      " 'train_f1': 0.6737967914438502,\n",
      " 'train_loss': 0.7546405792236328,\n",
      " 'train_precision': 0.525,\n",
      " 'train_recall': 0.9402985074626866}\n",
      "\n",
      "{'train_acc': 0.875,\n",
      " 'train_f1': 0.8873239436619719,\n",
      " 'train_loss': 0.30189383029937744,\n",
      " 'train_precision': 0.875,\n",
      " 'train_recall': 0.9}\n",
      "\n",
      "{'train_acc': 0.890625,\n",
      " 'train_f1': 0.8985507246376812,\n",
      " 'train_loss': 0.24497121572494507,\n",
      " 'train_precision': 0.8985507246376812,\n",
      " 'train_recall': 0.8985507246376812}\n",
      "\n",
      "{'train_acc': 0.8203125,\n",
      " 'train_f1': 0.8270676691729324,\n",
      " 'train_loss': 0.3666956424713135,\n",
      " 'train_precision': 0.859375,\n",
      " 'train_recall': 0.7971014492753623}\n",
      "\n",
      "{'train_acc': 0.8984375,\n",
      " 'train_f1': 0.8925619834710743,\n",
      " 'train_loss': 0.2723426818847656,\n",
      " 'train_precision': 0.9,\n",
      " 'train_recall': 0.8852459016393442}\n",
      "\n",
      "{'train_acc': 0.90625,\n",
      " 'train_f1': 0.9,\n",
      " 'train_loss': 0.2283640205860138,\n",
      " 'train_precision': 0.8852459016393442,\n",
      " 'train_recall': 0.9152542372881356}\n",
      "\n",
      "{'train_acc': 0.8984375,\n",
      " 'train_f1': 0.900763358778626,\n",
      " 'train_loss': 0.2045968770980835,\n",
      " 'train_precision': 0.8939393939393939,\n",
      " 'train_recall': 0.9076923076923077}\n",
      "\n",
      "{'train_acc': 0.9375,\n",
      " 'train_f1': 0.9420289855072465,\n",
      " 'train_loss': 0.16358816623687744,\n",
      " 'train_precision': 0.9558823529411765,\n",
      " 'train_recall': 0.9285714285714286}\n",
      "\n",
      "{'train_acc': 0.8828125,\n",
      " 'train_f1': 0.8818897637795275,\n",
      " 'train_loss': 0.26709672808647156,\n",
      " 'train_precision': 0.9180327868852459,\n",
      " 'train_recall': 0.8484848484848485}\n",
      "\n",
      "{'train_acc': 0.921875,\n",
      " 'train_f1': 0.9315068493150684,\n",
      " 'train_loss': 0.22758524119853973,\n",
      " 'train_precision': 0.9714285714285714,\n",
      " 'train_recall': 0.8947368421052632}\n",
      "\n",
      "{'train_acc': 0.9140625,\n",
      " 'train_f1': 0.9261744966442952,\n",
      " 'train_loss': 0.2210102081298828,\n",
      " 'train_precision': 0.9583333333333334,\n",
      " 'train_recall': 0.8961038961038961}\n",
      "\n",
      "{'train_acc': 0.84375,\n",
      " 'train_f1': 0.8461538461538461,\n",
      " 'train_loss': 0.365139365196228,\n",
      " 'train_precision': 0.859375,\n",
      " 'train_recall': 0.8333333333333334}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'val_acc': 0.89884,\n",
      " 'val_f1': 0.9010756894191276,\n",
      " 'val_loss': 0.24629278481006622,\n",
      " 'val_precision': 0.8874677351003583,\n",
      " 'val_recall': 0.9151074564017002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using PyTorch Ver\", torch.__version__)\n",
    "print(\"Fix Seed:\", args.random_seed)\n",
    "seed_everything(args.random_seed)\n",
    "model = Model(args)\n",
    "\n",
    "print(\":: Start Training ::\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        accelerator=\"auto\",\n",
    "        #accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        #strategy=\"ddp_notebook\",\n",
    "        devices=torch.cuda.device_count() if torch.cuda.is_available() else None,\n",
    "        #devices=\"auto\",\n",
    "        precision=16 if args.fp16 else 32,\n",
    "        # For TPU Setup\n",
    "        # tpu_cores=args.tpu_cores if args.tpu_cores else None,\n",
    "        logger=CSVLogger(save_dir=\"logs/\")\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed02c1-9344-44c8-b87f-f1e08d5b3785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
